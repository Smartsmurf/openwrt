diff -Nur linux-master/drivers/crypto/gemini/Makefile linux-new/drivers/crypto/gemini/Makefile
--- linux-master/drivers/crypto/gemini/Makefile	2023-02-19 13:24:22.000000000 +0100
+++ linux-new/drivers/crypto/gemini/Makefile	2023-04-13 07:26:35.000000000 +0200
@@ -1,2 +1,2 @@
 obj-$(CONFIG_CRYPTO_DEV_SL3516) += sl3516-ce.o
-sl3516-ce-y += sl3516-ce-core.o sl3516-ce-cipher.o sl3516-ce-rng.o
+sl3516-ce-y += sl3516-ce-core.o sl3516-ce-cipher.o sl3516-ce-rng.o sl3516-ce-ahash.o
diff -Nur linux-master/drivers/crypto/gemini/sl3516-ce.h linux-new/drivers/crypto/gemini/sl3516-ce.h
--- linux-master/drivers/crypto/gemini/sl3516-ce.h	2023-02-19 13:24:22.000000000 +0100
+++ linux-new/drivers/crypto/gemini/sl3516-ce.h	2023-04-13 09:41:12.816668600 +0200
@@ -3,6 +3,7 @@
  * sl3516-ce.h - hardware cryptographic offloader for cortina/gemini SoC
  *
  * Copyright (C) 2021 Corentin LABBE <clabbe@baylibre.com>
+ * Copyright (C) 2023 Andreas Fiedler <andreas.fiedler@gmx.net>
  *
  * General notes on this driver:
  * Called either Crypto Acceleration Engine Module, Security Acceleration Engine
@@ -14,12 +15,20 @@
  * It acts the same as a network hw, with both RX and TX chained descriptors.
  */
 #include <crypto/aes.h>
+#include <crypto/des.h>
 #include <crypto/engine.h>
 #include <crypto/scatterwalk.h>
 #include <crypto/skcipher.h>
 #include <linux/crypto.h>
 #include <linux/debugfs.h>
 #include <linux/hw_random.h>
+#include <crypto/hash.h>
+#include <crypto/md5.h>
+#include <crypto/sha1.h>
+#include <crypto/internal/des.h>
+#include <crypto/internal/rng.h>
+#include <crypto/internal/hash.h>
+#include <crypto/internal/skcipher.h>
 
 #define TQ0_TYPE_DATA 0
 #define TQ0_TYPE_CTRL BIT(0)
@@ -33,8 +42,20 @@
 #define TQ8_AKEY2 BIT(8)
 #define TQ9_AKEY2 BIT(9)
 
+#define ECB_DES       0x0
+#define ECB_3DES      0x1 
 #define ECB_AES       0x2
+#define CBC_DES       0x4
+#define CBC_3DES      0x5 
+#define CBC_AES       0x6
+
+#define CE_SHA1       0x0
+#define CE_MD5        0x1
+#define CE_HMAC_SHA1  0x2
+#define CE_HMAC_MD5   0x3
+#define CE_FCS        0x4
 
+#define DESC_LINK 0x00
 #define DESC_LAST 0x01
 #define DESC_FIRST 0x02
 
@@ -52,16 +73,20 @@
 #define IPSEC_TXDMA_BUF_ADDR	0xff28
 #define IPSEC_RXDMA_BUF_ADDR	0xff38
 #define IPSEC_RXDMA_BUF_SIZE	0xff30
-
 #define CE_ENCRYPTION		0x01
 #define CE_DECRYPTION		0x03
+#define CE_AUTHENTICATION	0x04
+#define CE_AUTH_APPEND		0x0
+#define CE_AUTH_CHECK		0x1
 
 #define MAXDESC 6
 
 #define DMA_STATUS_RS_EOFI	BIT(22)
+#define DMA_STATUS_RS_EODI	BIT(23)
 #define DMA_STATUS_RS_PERR	BIT(24)
 #define DMA_STATUS_RS_DERR	BIT(25)
 #define DMA_STATUS_TS_EOFI	BIT(27)
+#define DMA_STATUS_TS_EODI	BIT(28)
 #define DMA_STATUS_TS_PERR	BIT(29)
 #define DMA_STATUS_TS_DERR	BIT(30)
 
@@ -70,8 +95,13 @@
 #define TXDMA_CTRL_CHAIN_MODE BIT(29)
 /* the burst value is not documented in the datasheet */
 #define TXDMA_CTRL_BURST_UNK BIT(22)
+#define TXDMA_CTRL_INT_FINISH BIT(18)
 #define TXDMA_CTRL_INT_FAIL BIT(17)
 #define TXDMA_CTRL_INT_PERR BIT(16)
+#define TXDMA_CTRL_INT_EOD BIT(15)
+#define TXDMA_CTRL_INT_EOF BIT(14)
+
+#define RXDMA_FIRST_DESC_BUSY BIT(3)
 
 #define RXDMA_CTRL_START BIT(31)
 #define RXDMA_CTRL_CONTINUE BIT(30)
@@ -190,6 +220,11 @@
 	u32 header_len		:16;
 };
 
+struct pkt_control_auth {
+	u32 algorithm_len	:16;
+	u32 header_len		:16;
+};
+
 /*
  * struct pkt_control_ecb - control packet for ECB
  */
@@ -200,6 +235,35 @@
 };
 
 /*
+ * struct pkt_control_cbc - control packet for CBC
+ */
+struct pkt_control_cbc {
+	struct pkt_control_header control;
+	struct pkt_control_cipher cipher;
+	unsigned char iv[AES_BLOCK_SIZE];
+	unsigned char key[AES_MAX_KEY_SIZE];
+};
+
+/*
+ * struct pkt_control_hash - control packet for hashes
+ */
+struct pkt_control_hash {
+	struct pkt_control_header control;
+	struct pkt_control_auth auth;
+};
+
+/*
+ * struct pkt_control - control packet for all control frames
+ */
+struct pkt_control {
+	union {
+		struct pkt_control_ecb ecb;
+		struct pkt_control_cbc cbc;
+		struct pkt_control_hash hash;
+	} frame;
+};
+
+/*
  * struct sl3516_ce_dev - main container for all this driver information
  * @base:	base address
  * @clks:	clocks used
@@ -251,13 +315,15 @@
 	unsigned long fallback_sg_count_tx;
 	unsigned long fallback_sg_count_rx;
 	unsigned long fallback_not_same_len;
+	unsigned long fallback_len0;
 	unsigned long fallback_mod16;
 	unsigned long fallback_align16;
+	unsigned long fallback_other;
 #ifdef CONFIG_CRYPTO_DEV_SL3516_DEBUG
 	struct dentry *dbgfs_dir;
 	struct dentry *dbgfs_stats;
 #endif
-	void *pctrl;
+	struct pkt_control *pctrl;
 	dma_addr_t dctrl;
 };
 
@@ -273,7 +339,6 @@
  * @op_dir:		direction (encrypt vs decrypt) for this request
  * @pctrllen:		the length of the ctrl packet
  * @tqflag:		the TQflag to set in data packet
- * @h			pointer to the pkt_control_cipher header
  * @nr_sgs:		number of source SG
  * @nr_sgd:		number of destination SG
  * @fallback_req:	request struct for invoking the fallback skcipher TFM
@@ -284,10 +349,12 @@
 	u32 op_dir;
 	unsigned int pctrllen;
 	u32 tqflag;
-	struct pkt_control_cipher *h;
 	int nr_sgs;
 	int nr_sgd;
-	struct skcipher_request fallback_req;   // keep at the end
+	union {			// keep at the end
+		struct skcipher_request skcipher;
+		struct ahash_request ahash;
+	} fallback_req;
 };
 
 /*
@@ -302,10 +369,22 @@
  */
 struct sl3516_ce_cipher_tfm_ctx {
 	struct crypto_engine_ctx enginectx;
-	u32 *key;
-	u32 keylen;
+	union {
+		struct {
+			u32 *key;
+			u32 keylen;
+		} cipher;
+		struct {
+			int hasdigest;
+			u8 digest[SHA1_DIGEST_SIZE];
+		} hash;
+	};
 	struct sl3516_ce_dev *ce;
-	struct crypto_skcipher *fallback_tfm;
+	int force_fallback;
+	union {
+		struct crypto_skcipher *skcipher;
+		struct crypto_ahash *ahash;
+	} fallback_tfm;
 };
 
 /*
@@ -325,16 +404,28 @@
 	struct sl3516_ce_dev *ce;
 	union {
 		struct skcipher_alg skcipher;
+		struct ahash_alg hash;
 	} alg;
 	unsigned long stat_req;
 	unsigned long stat_fb;
 	unsigned long stat_bytes;
+	unsigned long stat_fb_len0;
 };
 
-int sl3516_ce_enqueue(struct crypto_async_request *areq, u32 type);
+int sl3516_ce_hash_crainit(struct crypto_tfm *tfm);
+void sl3516_ce_hash_craexit(struct crypto_tfm *tfm);
+int sl3516_ce_hash_init(struct ahash_request *areq);
+int sl3516_ce_hash_update(struct ahash_request *areq);
+int sl3516_ce_hash_final(struct ahash_request *areq);
+int sl3516_ce_hash_finup(struct ahash_request *areq);
+int sl3516_ce_hash_digest(struct ahash_request *areq);
+int sl3516_ce_hash_export(struct ahash_request *areq, void *out);
+int sl3516_ce_hash_import(struct ahash_request *areq, const void *in);
 
 int sl3516_ce_aes_setkey(struct crypto_skcipher *tfm, const u8 *key,
 			 unsigned int keylen);
+int sl3516_ce_des_setkey(struct crypto_skcipher *tfm, const u8 *key,
+			 unsigned int keylen);
 int sl3516_ce_cipher_init(struct crypto_tfm *tfm);
 void sl3516_ce_cipher_exit(struct crypto_tfm *tfm);
 int sl3516_ce_skdecrypt(struct skcipher_request *areq);
diff -Nur linux-master/drivers/crypto/gemini/sl3516-ce-ahash.c linux-new/drivers/crypto/gemini/sl3516-ce-ahash.c
--- linux-master/drivers/crypto/gemini/sl3516-ce-ahash.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-new/drivers/crypto/gemini/sl3516-ce-ahash.c	2023-04-13 09:14:06.439123300 +0200
@@ -0,0 +1,481 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * sl3516-ce-ahash.c - hardware cryptographic offloader for Storlink SL3516 SoC
+ *
+ * Copyright (C) 2023 Andreas FIEDLER <andreas.fiedler@gmx.net>
+ *
+ * This file adds support for MD5, SHA1 hashes 
+ *
+ * The operation of IPSEC is limited in a way that it can process a single
+ * digest only and return its result. It is neither possible to read or set
+ * the internal hash state, nor is it possible to run multiple updates since
+ * algorithm_len must be set in the first descriptor leading to the padding
+ * immediately after the first data block.
+ */
+
+#include <linux/crypto.h>
+#include <linux/dma-mapping.h>
+#include <linux/delay.h>
+#include <linux/io.h>
+#include <linux/pm_runtime.h>
+#include <crypto/scatterwalk.h>
+#include <crypto/internal/skcipher.h>
+#include "sl3516-ce.h"
+
+int process_id = 0;
+
+/* sl3516_ce_hash_need_fallback - check if a request can be handled by the CE */
+/* if fallback is required, set force_fallback flag for following calls in this */
+/* context, e.g. chain of import(fallback)/update(force)/final(force) */ 
+static bool sl3516_ce_hash_need_fallback(struct ahash_request *areq, int chklen)
+{
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sl3516_ce_cipher_tfm_ctx *op = crypto_ahash_ctx(tfm);
+	struct sl3516_ce_dev *ce = op->ce;
+	struct scatterlist *sg;
+
+	if (op->force_fallback) {
+		dev_dbg(ce->dev, "%s : force_fallback\n", __func__);
+		ce->fallback_other++;
+		return true;
+	}
+
+	/* IPSEC does not handle zero messages */
+	if (chklen && areq->nbytes == 0) {
+		dev_dbg(ce->dev, "%s : nbytes == 0\n", __func__);		
+		ce->fallback_not_same_len++;
+		goto need_fallback;
+	}
+
+	/* algorithm_len is 16 bits, but HASH engine breaks on len > PAGE_SIZE */
+	if (chklen && areq->nbytes >= PAGE_SIZE) {
+		dev_dbg(ce->dev, "%s : nbytes > PAGE_SIZE\n", __func__);		
+		ce->fallback_sg_count_tx++;
+		goto need_fallback;
+	}
+
+	if (sg_nents(areq->src) > MAXDESC ) {
+		dev_dbg(ce->dev, "%s : %i > MAXDESC\n", __func__, sg_nents(areq->src) );
+		ce->fallback_sg_count_tx++;
+		goto need_fallback;
+	}
+
+	sg = areq->src;
+	while (sg) {
+		if (!IS_ALIGNED(sg->offset, 16)) {
+			dev_info(ce->dev, "%s : sg->offset not aligned\n", __func__ );
+			ce->fallback_align16++;
+			goto need_fallback;
+		}
+		sg = sg_next(sg);
+	}
+	return false;
+need_fallback:
+	/* flag fallback for this context */
+	op->force_fallback = true;
+	return true;
+}
+
+/*
+ * hash run strategy:
+ * IPSEC [In]  - [input data]
+ *       [Out] - [input data <hash>]
+ * Returned data is written into scratch page allocated in this function.
+ * The last written page contains the digest immediately appended after
+ * the final byte of the input data.
+ */
+int sl3516_ce_hash_run(struct crypto_engine *engine, void *breq)
+{
+	struct ahash_request *areq = container_of(breq, struct ahash_request, base);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sl3516_ce_cipher_tfm_ctx *tctx = crypto_ahash_ctx(tfm);
+	struct sl3516_ce_cipher_req_ctx *rctx = ahash_request_ctx(areq);
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sl3516_ce_alg_template *algt;
+	struct sl3516_ce_dev *ce = tctx->ce;
+	struct scatterlist *sg;
+	unsigned int todo, len;
+	unsigned char *result = NULL;
+	dma_addr_t addr_res;
+	struct page *pdest;
+	int nr_sgs = 0;
+	int offset = 0;
+	int err = 0;
+	int digestsize; 
+	int i;
+ 
+	algt = container_of(alg, struct sl3516_ce_alg_template, alg.hash);
+	algt->stat_req++;
+
+	dev_dbg(ce->dev, "%s : %s\n", __func__, crypto_tfm_alg_name(areq->base.tfm) );
+
+	rctx->tqflag = TQ0_TYPE_CTRL;
+	rctx->tqflag |= TQ2_AUTH;
+
+	digestsize = crypto_ahash_digestsize(tfm);
+	switch (digestsize) {
+	case SHA1_DIGEST_SIZE:
+	case MD5_DIGEST_SIZE:
+		break;
+	default:
+		err =  -EINVAL;
+		goto theend;
+	}
+	nr_sgs = dma_map_sg(ce->dev, areq->src, sg_nents(areq->src),
+				DMA_TO_DEVICE);
+	if (nr_sgs <= 0 || nr_sgs > MAXDESC ) {
+		dev_err(ce->dev, "Invalid sg number %d\n", nr_sgs);
+		err = -EINVAL;
+		goto theend;
+	}
+	pdest = alloc_page(GFP_DMA);
+	if (!pdest) {
+		dev_err(ce->dev, "no memory for scratch page and digest\n");
+		err = -ENOMEM;
+		goto theend_sgs;
+	}
+	addr_res = dma_map_page(ce->dev, pdest, 0, PAGE_SIZE, DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(ce->dev, addr_res)) {
+		dev_err(ce->dev, "mapping temp page\n");
+		goto theend_sgs;
+	}
+
+	len = areq->nbytes; 
+	sg = areq->src;
+	i = 0;
+	while (i < nr_sgs && sg && len) {
+		if (sg_dma_len(sg) == 0)
+			goto sgs_next;
+		todo = min(len, sg_dma_len(sg));
+		rctx->t_src[i].addr = sg_dma_address(sg);
+		rctx->t_src[i].len = todo;
+		rctx->t_dst[i].addr = addr_res;
+		rctx->t_dst[i].len = todo;
+		dev_dbg(ce->dev, "%s : total=%u SGS(%d %u off=%d) todo=%u\n", __func__,
+			areq->nbytes, i, rctx->t_src[i].len, sg->offset, todo);
+		len -= todo;
+		i++;
+sgs_next:
+		sg = sg_next(sg);
+	}
+	nr_sgs = i;
+	if (len > 0) {
+		dev_err(ce->dev, "remaining len %d/%u nr_sgs=%d\n", len, areq->nbytes, nr_sgs);
+		err = -EINVAL;
+		goto theend_page;
+	}
+	i--;
+	offset = rctx->t_dst[i].len;
+	if ((offset+digestsize) > PAGE_SIZE) {
+		/* last segment cannot hold digest */
+		/* insert 1/2 page before final page */
+		rctx->t_dst[i].addr = addr_res;
+		rctx->t_dst[i++].len = PAGE_SIZE/2;
+		offset -= PAGE_SIZE/2;
+		rctx->t_dst[i].addr = addr_res;
+		rctx->t_dst[i].len = offset+digestsize;		
+		rctx->nr_sgd = nr_sgs+1;
+	} else {
+		rctx->t_dst[i].len += digestsize;		
+		rctx->nr_sgd = nr_sgs;
+	}
+	rctx->nr_sgs = nr_sgs;
+	
+	err = sl3516_ce_run_task(ce, rctx, crypto_tfm_alg_name(areq->base.tfm));
+
+	if (err == 0) {
+		result = kmap(pdest);
+		memcpy(tctx->hash.digest, result+offset, digestsize); 
+		tctx->hash.hasdigest = true;
+		kunmap(pdest);
+	}
+
+theend_page:
+	dma_unmap_page(ce->dev, addr_res, PAGE_SIZE, DMA_BIDIRECTIONAL);
+
+theend_sgs:
+	dma_unmap_sg(ce->dev, areq->src, sg_nents(areq->src),
+		     DMA_TO_DEVICE);
+
+theend:
+	__free_page(pdest);
+	local_bh_disable();
+	crypto_finalize_hash_request(ce->engine, areq, err);
+	local_bh_enable();
+	return 0;
+}
+
+int sl3516_ce_hash_crainit(struct crypto_tfm *tfm)
+{
+	struct sl3516_ce_cipher_tfm_ctx *tctx = crypto_tfm_ctx(tfm);
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->__crt_alg);
+	struct sl3516_ce_alg_template *algt;
+	const char *name = crypto_tfm_alg_name(tfm);
+	int err;
+
+	memset(tctx, 0, sizeof(struct sl3516_ce_cipher_tfm_ctx));
+
+	algt = container_of(alg, struct sl3516_ce_alg_template, alg.hash);
+	tctx->ce = algt->ce;
+
+	tctx->enginectx.op.do_one_request = sl3516_ce_hash_run;
+	tctx->enginectx.op.prepare_request = NULL;
+	tctx->enginectx.op.unprepare_request = NULL;
+
+	tctx->fallback_tfm.ahash = crypto_alloc_ahash(name, 0, CRYPTO_ALG_NEED_FALLBACK);
+	if (IS_ERR(tctx->fallback_tfm.ahash)) {
+		dev_err(tctx->ce->dev, "ERROR: Cannot allocate fallback for %s %ld\n",
+			name, PTR_ERR(tctx->fallback_tfm.ahash));
+		return PTR_ERR(tctx->fallback_tfm.ahash);
+	}
+
+	if (algt->alg.hash.halg.statesize < crypto_ahash_statesize(tctx->fallback_tfm.ahash))
+		algt->alg.hash.halg.statesize = crypto_ahash_statesize(tctx->fallback_tfm.ahash);
+
+	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),
+				 sizeof(struct sl3516_ce_cipher_req_ctx) +
+				 crypto_ahash_reqsize(tctx->fallback_tfm.ahash));
+
+	dev_dbg(tctx->ce->dev, "Fallback for %s is %s\n",
+		 name,
+		 crypto_tfm_alg_driver_name(&tctx->fallback_tfm.ahash->base));
+
+	err = pm_runtime_get_sync(tctx->ce->dev);
+	if (err < 0)
+		goto error_pm;
+
+	return 0;
+error_pm:
+	pm_runtime_put_noidle(tctx->ce->dev);
+	crypto_free_ahash(tctx->fallback_tfm.ahash);
+	return err;
+}
+
+void sl3516_ce_hash_craexit(struct crypto_tfm *tfm)
+{
+	struct sl3516_ce_cipher_tfm_ctx *op = crypto_tfm_ctx(tfm);
+
+	crypto_free_ahash(op->fallback_tfm.ahash);
+	pm_runtime_put_sync_suspend(op->ce->dev);
+}
+
+int sl3516_ce_hash_init(struct ahash_request *areq)
+{
+	struct sl3516_ce_cipher_req_ctx *rctx = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sl3516_ce_cipher_tfm_ctx *tctx = crypto_ahash_ctx(tfm);
+
+	memset(rctx, 0, sizeof(struct sl3516_ce_cipher_req_ctx));
+
+	ahash_request_set_tfm(&rctx->fallback_req.ahash, tctx->fallback_tfm.ahash);
+	rctx->fallback_req.ahash.base.flags = areq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;
+
+	return crypto_ahash_init(&rctx->fallback_req.ahash);
+
+}
+
+int sl3516_ce_hash_import(struct ahash_request *areq, const void *in)
+{
+	struct sl3516_ce_cipher_req_ctx *rctx = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sl3516_ce_cipher_tfm_ctx *tctx = crypto_ahash_ctx(tfm);
+
+	/* unable access CE hash state -> force fallback */
+	tctx->force_fallback = true;
+
+	ahash_request_set_tfm(&rctx->fallback_req.ahash, tctx->fallback_tfm.ahash);
+	rctx->fallback_req.ahash.base.flags = areq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;
+
+	return crypto_ahash_import(&rctx->fallback_req.ahash, in);
+}
+
+int sl3516_ce_hash_export(struct ahash_request *areq, void *out)
+{
+	struct sl3516_ce_cipher_req_ctx *rctx = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sl3516_ce_cipher_tfm_ctx *tctx = crypto_ahash_ctx(tfm);
+
+	/* unable access CE hash state -> force fallback */
+	tctx->force_fallback = true;
+
+	ahash_request_set_tfm(&rctx->fallback_req.ahash, tctx->fallback_tfm.ahash);
+	rctx->fallback_req.ahash.base.flags = areq->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP;
+
+	return crypto_ahash_export(&rctx->fallback_req.ahash, out);
+}
+
+int sl3516_ce_hash_update_fb(struct ahash_request *areq)
+{
+	struct sl3516_ce_cipher_req_ctx *rctx = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sl3516_ce_cipher_tfm_ctx *tctx = crypto_ahash_ctx(tfm);
+
+	struct sl3516_ce_cipher_tfm_ctx *op = crypto_ahash_ctx(tfm);
+	dev_dbg(op->ce->dev, "%s\n : areq = %p\n", __func__, areq);
+
+	ahash_request_set_tfm(&rctx->fallback_req.ahash, tctx->fallback_tfm.ahash);
+	rctx->fallback_req.ahash.base.flags = areq->base.flags &
+					CRYPTO_TFM_REQ_MAY_SLEEP;
+	rctx->fallback_req.ahash.nbytes = areq->nbytes;
+	rctx->fallback_req.ahash.src = areq->src;
+
+	return crypto_ahash_update(&rctx->fallback_req.ahash);
+}
+
+int sl3516_ce_hash_final_fb(struct ahash_request *areq)
+{
+	struct sl3516_ce_cipher_req_ctx *rctx = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sl3516_ce_cipher_tfm_ctx *tctx = crypto_ahash_ctx(tfm);
+
+#ifdef CONFIG_CRYPTO_DEV_SL3516_DEBUG
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sl3516_ce_alg_template *algt;
+#endif
+	struct sl3516_ce_cipher_tfm_ctx *op = crypto_ahash_ctx(tfm);
+	dev_dbg(op->ce->dev, "%s\n", __func__);
+
+	ahash_request_set_tfm(&rctx->fallback_req.ahash, tctx->fallback_tfm.ahash);
+	rctx->fallback_req.ahash.base.flags = areq->base.flags &
+					CRYPTO_TFM_REQ_MAY_SLEEP;
+	rctx->fallback_req.ahash.result = areq->result;
+
+#ifdef CONFIG_CRYPTO_DEV_SL3516_DEBUG
+	algt = container_of(alg, struct sl3516_ce_alg_template, alg.hash);
+	algt->stat_fb++;
+#endif
+
+	return crypto_ahash_final(&rctx->fallback_req.ahash);
+}
+
+int sl3516_ce_hash_final(struct ahash_request *areq)
+{
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sl3516_ce_cipher_tfm_ctx *tctx = crypto_ahash_ctx(tfm);
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sl3516_ce_alg_template *algt;
+
+	struct sl3516_ce_dev *ce = tctx->ce;
+	dev_dbg(ce->dev, "%s : areq = %p\n", __func__, areq);
+
+	if( !tctx->hash.hasdigest || sl3516_ce_hash_need_fallback(areq, false) )
+		return sl3516_ce_hash_final_fb(areq);
+
+	algt = container_of(alg, struct sl3516_ce_alg_template, alg.hash);
+
+	if( areq->result )
+		memcpy(areq->result, tctx->hash.digest, algt->alg.hash.halg.digestsize); 
+
+	return 0;
+	
+}
+
+int sl3516_ce_hash_finup(struct ahash_request *areq)
+{
+	struct sl3516_ce_cipher_req_ctx *rctx = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sl3516_ce_cipher_tfm_ctx *tctx = crypto_ahash_ctx(tfm);
+
+#ifdef CONFIG_CRYPTO_DEV_SL3516_DEBUG
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sl3516_ce_alg_template *algt;
+#endif
+	dev_dbg(tctx->ce->dev, "%s : areq = %p\n", __func__, areq);
+
+	ahash_request_set_tfm(&rctx->fallback_req.ahash, tctx->fallback_tfm.ahash);
+	rctx->fallback_req.ahash.base.flags = areq->base.flags &
+					CRYPTO_TFM_REQ_MAY_SLEEP;
+
+	rctx->fallback_req.ahash.nbytes = areq->nbytes;
+	rctx->fallback_req.ahash.src = areq->src;
+	rctx->fallback_req.ahash.result = areq->result;
+
+#ifdef CONFIG_CRYPTO_DEV_SL3516_DEBUG
+	algt = container_of(alg, struct sl3516_ce_alg_template, alg.hash);
+	algt->stat_fb++;
+#endif
+
+	return crypto_ahash_finup(&rctx->fallback_req.ahash);
+}
+
+static int sl3516_ce_hash_digest_fb(struct ahash_request *areq)
+{
+	struct sl3516_ce_cipher_req_ctx *rctx = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sl3516_ce_cipher_tfm_ctx *tctx = crypto_ahash_ctx(tfm);
+#ifdef CONFIG_CRYPTO_DEV_SL3516_DEBUG
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sl3516_ce_alg_template *algt;
+#endif
+	dev_dbg(tctx->ce->dev, "%s : areq = %p\n", __func__, areq);
+
+	ahash_request_set_tfm(&rctx->fallback_req.ahash, tctx->fallback_tfm.ahash);
+	rctx->fallback_req.ahash.base.flags = areq->base.flags &
+					CRYPTO_TFM_REQ_MAY_SLEEP;
+
+	rctx->fallback_req.ahash.nbytes = areq->nbytes;
+	rctx->fallback_req.ahash.src = areq->src;
+	rctx->fallback_req.ahash.result = areq->result;
+#ifdef CONFIG_CRYPTO_DEV_SL3516_DEBUG
+	algt = container_of(alg, struct sl3516_ce_alg_template, alg.hash);
+	algt->stat_fb++;
+#endif
+	return crypto_ahash_digest(&rctx->fallback_req.ahash);
+}
+
+int sl3516_ce_hash_update(struct ahash_request *areq)
+{
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sl3516_ce_cipher_tfm_ctx *tctx = crypto_ahash_ctx(tfm);
+	struct sl3516_ce_dev *ce = tctx->ce;
+
+	dev_dbg(ce->dev, "%s : areq = %p, len = %i\n", __func__, areq, areq->nbytes);
+
+	/* IPSEC processes complete blocks only -> no support for update */
+	tctx->force_fallback = true;
+	return sl3516_ce_hash_update_fb(areq);
+}
+
+int sl3516_ce_hash_digest(struct ahash_request *areq)
+{
+	struct sl3516_ce_cipher_req_ctx *rctx = ahash_request_ctx(areq);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(areq);
+	struct sl3516_ce_cipher_tfm_ctx *tctx = crypto_ahash_ctx(tfm);
+	struct sl3516_ce_dev *ce = tctx->ce;
+	struct ahash_alg *alg = __crypto_ahash_alg(tfm->base.__crt_alg);
+	struct sl3516_ce_alg_template *algt;
+	struct pkt_control_hash *hash;
+	int err;
+
+	dev_dbg(ce->dev, "%s : areq = %p, len = %i\n", __func__, areq, areq->nbytes);
+
+	if( sl3516_ce_hash_need_fallback(areq, true) )
+		return sl3516_ce_hash_digest_fb(areq);
+
+	algt = container_of(alg, struct sl3516_ce_alg_template, alg.hash);
+
+	rctx->pctrllen = sizeof(struct pkt_control_hash);
+	hash = &ce->pctrl->frame.hash;
+
+	hash->control.op_mode = CE_AUTHENTICATION;
+	hash->control.auth_mode = CE_AUTH_APPEND;
+	hash->control.process_id = 255 & (process_id++);
+
+	switch (algt->mode) {
+	case CE_MD5:
+		hash->control.auth_algorithm = CE_MD5;
+		hash->control.auth_check_len = MD5_DIGEST_SIZE / 4;
+		break;
+	case CE_SHA1:		
+		hash->control.auth_algorithm = CE_SHA1;
+		hash->control.auth_check_len = SHA1_DIGEST_SIZE / 4;
+		break;
+	}
+
+	hash->auth.header_len = 0;
+	hash->auth.algorithm_len = areq->nbytes;
+
+	err = crypto_transfer_hash_request_to_engine(ce->engine, areq);
+
+	return err;
+}
diff -Nur linux-master/drivers/crypto/gemini/sl3516-ce-cipher.c linux-new/drivers/crypto/gemini/sl3516-ce-cipher.c
--- linux-master/drivers/crypto/gemini/sl3516-ce-cipher.c	2023-02-19 13:24:22.000000000 +0100
+++ linux-new/drivers/crypto/gemini/sl3516-ce-cipher.c	2023-04-13 09:42:49.312391800 +0200
@@ -3,9 +3,10 @@
  * sl3516-ce-cipher.c - hardware cryptographic offloader for Storlink SL3516 SoC
  *
  * Copyright (C) 2021 Corentin LABBE <clabbe@baylibre.com>
+ * Copyright (C) 2023 Andreas FIEDLER <andreas.fiedler@gmx.net>
  *
- * This file adds support for AES cipher with 128,192,256 bits keysize in
- * ECB mode.
+ * This file adds support for AES cipher with 128,192,256 bits keysize
+*  and DES, 3DES cipher in both ECB and CBC mode.
  */
 
 #include <linux/crypto.h>
@@ -23,15 +24,16 @@
 	struct scatterlist *sg;
 
-	if (areq->cryptlen == 0 || areq->cryptlen % 16) {
+	if (areq->cryptlen == 0 || areq->cryptlen % 8) {
 		ce->fallback_mod16++;
 		return true;
 	}
 
 	/*
 	 * check if we have enough descriptors for TX
-	 * Note: TX need one control desc for each SG
+	 * Note: 1x TX needed for control desc, but
+	 * neither part of t_src nor t_dst
 	 */
-	if (sg_nents(areq->src) > MAXDESC / 2) {
+	if (sg_nents(areq->src) > MAXDESC) {
 		ce->fallback_sg_count_tx++;
 		return true;
 	}
@@ -48,11 +50,11 @@
 
 	sg = areq->src;
 	while (sg) {
-		if ((sg->length % 16) != 0) {
+		if ((sg->length % 8) != 0) {
 			ce->fallback_mod16++;
 			return true;
 		}
-		if ((sg_dma_len(sg) % 16) != 0) {
+		if ((sg_dma_len(sg) % 8) != 0) {
 			ce->fallback_mod16++;
 			return true;
 		}
@@ -64,11 +66,11 @@
 	}
 	sg = areq->dst;
 	while (sg) {
-		if ((sg->length % 16) != 0) {
+		if ((sg->length % 8) != 0) {
 			ce->fallback_mod16++;
 			return true;
 		}
-		if ((sg_dma_len(sg) % 16) != 0) {
+		if ((sg_dma_len(sg) % 8) != 0) {
 			ce->fallback_mod16++;
 			return true;
 		}
@@ -108,15 +110,15 @@
 	algt = container_of(alg, struct sl3516_ce_alg_template, alg.skcipher);
 	algt->stat_fb++;
 
-	skcipher_request_set_tfm(&rctx->fallback_req, op->fallback_tfm);
-	skcipher_request_set_callback(&rctx->fallback_req, areq->base.flags,
+	skcipher_request_set_tfm(&rctx->fallback_req.skcipher, op->fallback_tfm.skcipher);
+	skcipher_request_set_callback(&rctx->fallback_req.skcipher, areq->base.flags,
 				      areq->base.complete, areq->base.data);
-	skcipher_request_set_crypt(&rctx->fallback_req, areq->src, areq->dst,
+	skcipher_request_set_crypt(&rctx->fallback_req.skcipher, areq->src, areq->dst,
 				   areq->cryptlen, areq->iv);
 	if (rctx->op_dir == CE_DECRYPTION)
-		err = crypto_skcipher_decrypt(&rctx->fallback_req);
+		err = crypto_skcipher_decrypt(&rctx->fallback_req.skcipher);
 	else
-		err = crypto_skcipher_encrypt(&rctx->fallback_req);
+		err = crypto_skcipher_encrypt(&rctx->fallback_req.skcipher);
 	return err;
 }
 
@@ -131,10 +133,12 @@
 	struct scatterlist *sg;
 	unsigned int todo, len;
 	struct pkt_control_ecb *ecb;
+	struct pkt_control_cbc *cbc;
 	int nr_sgs = 0;
 	int nr_sgd = 0;
 	int err = 0;
 	int i;
+	unsigned int ivsize = crypto_skcipher_ivsize(tfm);
 
 	algt = container_of(alg, struct sl3516_ce_alg_template, alg.skcipher);
 
@@ -142,7 +146,7 @@
 		crypto_tfm_alg_name(areq->base.tfm),
 		areq->cryptlen,
 		rctx->op_dir, areq->iv, crypto_skcipher_ivsize(tfm),
-		op->keylen);
+		op->cipher.keylen);
 
 	algt->stat_req++;
 
@@ -217,24 +221,46 @@
 		goto theend_sgs;
 	}
 
+	rctx->tqflag = TQ0_TYPE_CTRL;
+	rctx->tqflag |= TQ1_CIPHER;
+
+	rctx->tqflag |= TQ4_KEY0;
+	rctx->tqflag |= TQ5_KEY4;
+	rctx->tqflag |= TQ6_KEY6;
+
 	switch (algt->mode) {
 	case ECB_AES:
+	case ECB_DES:
+	case ECB_3DES:
 		rctx->pctrllen = sizeof(struct pkt_control_ecb);
-		ecb = (struct pkt_control_ecb *)ce->pctrl;
-
-		rctx->tqflag = TQ0_TYPE_CTRL;
-		rctx->tqflag |= TQ1_CIPHER;
+		ecb = &ce->pctrl->frame.ecb;
 		ecb->control.op_mode = rctx->op_dir;
-		ecb->control.cipher_algorithm = ECB_AES;
+		ecb->control.cipher_algorithm = algt->mode;
 		ecb->cipher.header_len = 0;
 		ecb->cipher.algorithm_len = areq->cryptlen;
-		cpu_to_be32_array((__be32 *)ecb->key, (u32 *)op->key, op->keylen / 4);
-		rctx->h = &ecb->cipher;
-
-		rctx->tqflag |= TQ4_KEY0;
-		rctx->tqflag |= TQ5_KEY4;
-		rctx->tqflag |= TQ6_KEY6;
-		ecb->control.aesnk = op->keylen / 4;
+		cpu_to_be32_array((__be32 *)ecb->key, (u32 *)op->cipher.key, op->cipher.keylen / 4);
+		if (algt->mode == ECB_AES)
+			ecb->control.aesnk = op->cipher.keylen / 4;
+		break;
+	case CBC_AES:
+	case CBC_DES:
+	case CBC_3DES:
+		rctx->tqflag |= TQ3_IV;
+		rctx->pctrllen = sizeof(struct pkt_control_cbc);
+		cbc = &ce->pctrl->frame.cbc;
+		cbc->control.op_mode = rctx->op_dir;
+		cbc->control.cipher_algorithm = algt->mode;
+		cbc->cipher.header_len = 0;
+		cbc->cipher.algorithm_len = areq->cryptlen;
+		cpu_to_be32_array((__be32 *)cbc->key, (u32 *)op->cipher.key, op->cipher.keylen / 4);
+		cpu_to_be32_array((__be32 *)cbc->iv, (u32 *)areq->iv, ivsize / 4);
+		if (algt->mode == CBC_AES)
+			cbc->control.aesnk = op->cipher.keylen / 4;
+		/* save and return IV for chaining */
+		if (rctx->op_dir == CE_DECRYPTION)
+			scatterwalk_map_and_copy(areq->iv, areq->src,
+						 areq->cryptlen - ivsize,
+						 ivsize, 0);
 		break;
 	}
 
@@ -242,6 +268,12 @@
 	rctx->nr_sgd = nr_sgd;
 	err = sl3516_ce_run_task(ce, rctx, crypto_tfm_alg_name(areq->base.tfm));
 
+	/* copy IV or last block back for chaining */
+	if (ivsize > 0) {
+		if (rctx->op_dir == CE_ENCRYPTION)
+			scatterwalk_map_and_copy(areq->iv, areq->dst, 
+				areq->cryptlen - ivsize, ivsize, 0); 
+	}
 theend_sgs:
 	if (areq->src == areq->dst) {
 		dma_unmap_sg(ce->dev, areq->src, sg_nents(areq->src),
@@ -321,19 +353,19 @@
 	algt = container_of(alg, struct sl3516_ce_alg_template, alg.skcipher);
 	op->ce = algt->ce;
 
-	op->fallback_tfm = crypto_alloc_skcipher(name, 0, CRYPTO_ALG_NEED_FALLBACK);
-	if (IS_ERR(op->fallback_tfm)) {
+	op->fallback_tfm.skcipher = crypto_alloc_skcipher(name, 0, CRYPTO_ALG_NEED_FALLBACK);
+	if (IS_ERR(op->fallback_tfm.skcipher)) {
 		dev_err(op->ce->dev, "ERROR: Cannot allocate fallback for %s %ld\n",
-			name, PTR_ERR(op->fallback_tfm));
-		return PTR_ERR(op->fallback_tfm);
+			name, PTR_ERR(op->fallback_tfm.skcipher));
+		return PTR_ERR(op->fallback_tfm.skcipher);
 	}
 
 	sktfm->reqsize = sizeof(struct sl3516_ce_cipher_req_ctx) +
-			 crypto_skcipher_reqsize(op->fallback_tfm);
+			 crypto_skcipher_reqsize(op->fallback_tfm.skcipher);
 
-	dev_info(op->ce->dev, "Fallback for %s is %s\n",
+	dev_dbg(op->ce->dev, "Fallback for %s is %s\n",
 		 crypto_tfm_alg_driver_name(&sktfm->base),
-		 crypto_tfm_alg_driver_name(crypto_skcipher_tfm(op->fallback_tfm)));
+		 crypto_tfm_alg_driver_name(crypto_skcipher_tfm(op->fallback_tfm.skcipher)));
 
 	op->enginectx.op.do_one_request = sl3516_ce_handle_cipher_request;
 	op->enginectx.op.prepare_request = NULL;
@@ -346,7 +378,7 @@
 	return 0;
 error_pm:
 	pm_runtime_put_noidle(op->ce->dev);
-	crypto_free_skcipher(op->fallback_tfm);
+	crypto_free_skcipher(op->fallback_tfm.skcipher);
 	return err;
 }
 
@@ -354,8 +386,8 @@
 {
 	struct sl3516_ce_cipher_tfm_ctx *op = crypto_tfm_ctx(tfm);
 
-	kfree_sensitive(op->key);
-	crypto_free_skcipher(op->fallback_tfm);
+	kfree_sensitive(op->cipher.key);
+	crypto_free_skcipher(op->fallback_tfm.skcipher);
 	pm_runtime_put_sync_suspend(op->ce->dev);
 }
 
@@ -373,17 +405,60 @@
 	case 256 / 8:
 		break;
 	default:
-		dev_dbg(ce->dev, "ERROR: Invalid keylen %u\n", keylen);
+		dev_err(ce->dev, "ERROR: Invalid keylen %u\n", keylen);
 		return -EINVAL;
 	}
-	kfree_sensitive(op->key);
-	op->keylen = keylen;
-	op->key = kmemdup(key, keylen, GFP_KERNEL | GFP_DMA);
-	if (!op->key)
+	kfree_sensitive(op->cipher.key);
+	op->cipher.keylen = keylen;
+	op->cipher.key = kmemdup(key, keylen, GFP_KERNEL | GFP_DMA);
+	if (!op->cipher.key)
 		return -ENOMEM;
 
-	crypto_skcipher_clear_flags(op->fallback_tfm, CRYPTO_TFM_REQ_MASK);
-	crypto_skcipher_set_flags(op->fallback_tfm, tfm->base.crt_flags & CRYPTO_TFM_REQ_MASK);
+	crypto_skcipher_clear_flags(op->fallback_tfm.skcipher, CRYPTO_TFM_REQ_MASK);
+	crypto_skcipher_set_flags(op->fallback_tfm.skcipher, tfm->base.crt_flags & CRYPTO_TFM_REQ_MASK);
+
+	return crypto_skcipher_setkey(op->fallback_tfm.skcipher, key, keylen);
+}
+
+int sl3516_ce_des_setkey(struct crypto_skcipher *tfm, const u8 *key,
+			 unsigned int keylen)
+{
+	struct sl3516_ce_cipher_tfm_ctx *op = crypto_skcipher_ctx(tfm);
+	struct skcipher_alg *alg = crypto_skcipher_alg(tfm);
+	struct sl3516_ce_alg_template *algt;
+	struct sl3516_ce_dev *ce = op->ce;
+	int err = -1;
+
+	if (keylen != DES_KEY_SIZE && keylen != DES3_EDE_KEY_SIZE) {
+		dev_err(ce->dev, "ERROR: Invalid keylen %u\n", keylen);
+		return -EINVAL;
+	}
+	
+	algt = container_of(alg, struct sl3516_ce_alg_template, alg.skcipher);
+
+	switch (algt->mode) {
+	case CBC_DES:
+	case ECB_DES:
+		err = verify_skcipher_des_key(tfm, key); 
+		break;
+	case CBC_3DES:
+	case ECB_3DES:
+		err = verify_skcipher_des3_key(tfm, key); 
+		break;
+	}
+	if (err)
+		return err;
+
+	kfree_sensitive(op->cipher.key);
+
+	op->cipher.keylen = keylen;
+	op->cipher.key = kmemdup(key, keylen, GFP_KERNEL | GFP_DMA);
+	if (!op->cipher.key)
+		return -ENOMEM;
+
+	crypto_skcipher_clear_flags(op->fallback_tfm.skcipher, CRYPTO_TFM_REQ_MASK);
+	crypto_skcipher_set_flags(op->fallback_tfm.skcipher, tfm->base.crt_flags & CRYPTO_TFM_REQ_MASK);
+
+	return crypto_skcipher_setkey(op->fallback_tfm.skcipher, key, keylen);
 
-	return crypto_skcipher_setkey(op->fallback_tfm, key, keylen);
 }
diff -Nur linux-master/drivers/crypto/gemini/sl3516-ce-core.c linux-new/drivers/crypto/gemini/sl3516-ce-core.c
--- linux-master/drivers/crypto/gemini/sl3516-ce-core.c	2023-02-19 13:24:22.000000000 +0100
+++ linux-new/drivers/crypto/gemini/sl3516-ce-core.c	2023-04-13 09:47:39.184884000 +0200
@@ -3,8 +3,10 @@
  * sl3516-ce-core.c - hardware cryptographic offloader for Storlink SL3516 SoC
  *
  * Copyright (C) 2021 Corentin Labbe <clabbe@baylibre.com>
+ * Copyright (C) 2023 Andreas Fiedler <andreas.fiedler@gmx.net>
  *
  * Core file which registers crypto algorithms supported by the CryptoEngine
+ * and provides base communication with IPSEC engine
  */
 #include <linux/clk.h>
 #include <linux/crypto.h>
@@ -20,8 +22,6 @@
 #include <linux/platform_device.h>
 #include <linux/pm_runtime.h>
 #include <linux/reset.h>
-#include <crypto/internal/rng.h>
-#include <crypto/internal/skcipher.h>
 
 #include "sl3516-ce.h"
 
@@ -49,7 +49,7 @@
 	}
 	ce->rx[MAXDESC - 1].next_desc.next_descriptor = ce->drx;
 
-	ce->pctrl = dma_alloc_coherent(ce->dev, sizeof(struct pkt_control_ecb),
+	ce->pctrl = dma_alloc_coherent(ce->dev, sizeof(struct pkt_control),
 				       &ce->dctrl, GFP_KERNEL);
 	if (!ce->pctrl)
 		goto err_pctrl;
@@ -68,7 +68,7 @@
 
 	dma_free_coherent(ce->dev, sz, ce->tx, ce->dtx);
 	dma_free_coherent(ce->dev, sz, ce->rx, ce->drx);
-	dma_free_coherent(ce->dev, sizeof(struct pkt_control_ecb), ce->pctrl,
+	dma_free_coherent(ce->dev, sizeof(struct pkt_control), ce->pctrl,
 			  ce->dctrl);
 }
 
@@ -121,39 +121,48 @@
 {
 	struct descriptor *dd, *rdd = NULL;
 	u32 v;
-	int i, err = 0;
+	int i, err = 0, ctx = 0, crx = 0;
 
 	ce->stat_req++;
 
 	reinit_completion(&ce->complete);
 	ce->status = 0;
 
+	crx = ce->crx;
+	ctx = ce->ctx;
+	
 	for (i = 0; i < rctx->nr_sgd; i++) {
 		dev_dbg(ce->dev, "%s handle DST SG %d/%d len=%d\n", __func__,
 			i, rctx->nr_sgd, rctx->t_dst[i].len);
 		rdd = get_desc_rx(ce);
 		rdd->buf_adr = rctx->t_dst[i].addr;
 		rdd->frame_ctrl.bits.buffer_size = rctx->t_dst[i].len;
+		if (i > 0) {
+			rdd->next_desc.bits.sof_eof = DESC_LINK;
+		} else {
+			rdd->next_desc.bits.sof_eof = DESC_FIRST;
+		}
 		rdd->frame_ctrl.bits.own = CE_DMA;
 	}
 	rdd->next_desc.bits.eofie = 1;
+	rdd->next_desc.bits.sof_eof |= DESC_LAST; 
+
+	/* send control frame */
+	dd = get_desc_tx(ce);
+	dd->frame_ctrl.raw = 0;
+	dd->flag_status.raw = 0;
+	dd->frame_ctrl.bits.buffer_size = rctx->pctrllen;
+	dd->buf_adr = ce->dctrl;
+	dd->flag_status.tx_flag.tqflag = rctx->tqflag;
+	dd->next_desc.bits.eofie = 0;
+	dd->next_desc.bits.dec = 0;
+	dd->next_desc.bits.sof_eof = DESC_FIRST | DESC_LAST;
+	dd->frame_ctrl.bits.own = CE_DMA;
 
+	/* send all data packets in one data frame */
 	for (i = 0; i < rctx->nr_sgs; i++) {
 		dev_dbg(ce->dev, "%s handle SRC SG %d/%d len=%d\n", __func__,
 			i, rctx->nr_sgs, rctx->t_src[i].len);
-		rctx->h->algorithm_len = rctx->t_src[i].len;
-
-		dd = get_desc_tx(ce);
-		dd->frame_ctrl.raw = 0;
-		dd->flag_status.raw = 0;
-		dd->frame_ctrl.bits.buffer_size = rctx->pctrllen;
-		dd->buf_adr = ce->dctrl;
-		dd->flag_status.tx_flag.tqflag = rctx->tqflag;
-		dd->next_desc.bits.eofie = 0;
-		dd->next_desc.bits.dec = 0;
-		dd->next_desc.bits.sof_eof = DESC_FIRST | DESC_LAST;
-		dd->frame_ctrl.bits.own = CE_DMA;
-
 		dd = get_desc_tx(ce);
 		dd->frame_ctrl.raw = 0;
 		dd->flag_status.raw = 0;
@@ -162,13 +171,33 @@
 		dd->flag_status.tx_flag.tqflag = 0;
 		dd->next_desc.bits.eofie = 0;
 		dd->next_desc.bits.dec = 0;
-		dd->next_desc.bits.sof_eof = DESC_FIRST | DESC_LAST;
+		if (i > 0) {
+			dd->next_desc.bits.sof_eof = DESC_LINK;
+		} else {
+			dd->next_desc.bits.sof_eof = DESC_FIRST;
+		}
 		dd->frame_ctrl.bits.own = CE_DMA;
-		start_dma_tx(ce);
-		start_dma_rx(ce);
 	}
+	dd->next_desc.bits.sof_eof |= DESC_LAST; 
+
+	start_dma_rx(ce);
+	start_dma_tx(ce);
+
 	wait_for_completion_interruptible_timeout(&ce->complete,
 						  msecs_to_jiffies(5000));
+
+	/* set ownership for RX/TX data packets back to CPU */
+	/* DMA sets frame control and first data descriptor */
+	/* only (BUG?) */
+	for (i = 0; i <= rctx->nr_sgs;i++) {
+		dd = &ce->tx[(ctx+i) % MAXDESC];
+		dd->frame_ctrl.bits.own = CE_CPU;
+	}
+	for (i = 0; i <= rctx->nr_sgd;i++) {
+		dd = &ce->rx[(crx+i) % MAXDESC];
+		dd->frame_ctrl.bits.own = CE_CPU;
+	}
+
 	if (ce->status == 0) {
 		dev_err(ce->dev, "DMA timeout for %s\n", name);
 		err = -EFAULT;
@@ -214,6 +243,66 @@
 }
 
 static struct sl3516_ce_alg_template ce_algs[] = {
+{       .type = CRYPTO_ALG_TYPE_AHASH,
+	.mode = CE_MD5,
+	.alg.hash = {
+		.init = sl3516_ce_hash_init,
+		.update = sl3516_ce_hash_update,
+		.final = sl3516_ce_hash_final,
+		.finup = sl3516_ce_hash_finup,
+		.digest = sl3516_ce_hash_digest,
+		.export = sl3516_ce_hash_export,
+		.import = sl3516_ce_hash_import,
+		.halg = {
+			.digestsize = MD5_DIGEST_SIZE,
+			.statesize = sizeof(struct md5_state),
+			.base = {
+				.cra_name = "md5",
+				.cra_driver_name = "md5-sl3516",
+				.cra_priority = 300,
+				.cra_alignmask = 0xf,
+				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
+					CRYPTO_ALG_ASYNC | CRYPTO_ALG_KERN_DRIVER_ONLY |
+					CRYPTO_ALG_NEED_FALLBACK,
+				.cra_blocksize = MD5_HMAC_BLOCK_SIZE,
+				.cra_ctxsize = sizeof(struct sl3516_ce_cipher_tfm_ctx),
+				.cra_module = THIS_MODULE,
+				.cra_init = sl3516_ce_hash_crainit,
+				.cra_exit = sl3516_ce_hash_craexit,
+			}
+		}
+	}
+},
+{       .type = CRYPTO_ALG_TYPE_AHASH,
+	.mode = CE_SHA1,
+	.alg.hash = {
+		.init = sl3516_ce_hash_init,
+		.update = sl3516_ce_hash_update,
+		.final = sl3516_ce_hash_final,
+		.finup = sl3516_ce_hash_finup,
+		.digest = sl3516_ce_hash_digest,
+		.export = sl3516_ce_hash_export,
+		.import = sl3516_ce_hash_import,
+		.halg = {
+			.digestsize = SHA1_DIGEST_SIZE,
+			.statesize = sizeof(struct sha1_state),
+			.base = {
+				.cra_name = "sha1",
+				.cra_driver_name = "sha1-sl3516",
+				.cra_priority = 300,
+				.cra_alignmask = 0xf,
+				.cra_flags = CRYPTO_ALG_TYPE_AHASH |
+					CRYPTO_ALG_ASYNC | CRYPTO_ALG_KERN_DRIVER_ONLY |
+					CRYPTO_ALG_NEED_FALLBACK,
+				.cra_blocksize = SHA1_BLOCK_SIZE,
+				.cra_ctxsize = sizeof(struct sl3516_ce_cipher_tfm_ctx),
+				.cra_module = THIS_MODULE,
+				.cra_init = sl3516_ce_hash_crainit,
+				.cra_exit = sl3516_ce_hash_craexit,
+			}
+		}
+	}
+},
 {
 	.type = CRYPTO_ALG_TYPE_SKCIPHER,
 	.mode = ECB_AES,
@@ -223,7 +312,31 @@
 			.cra_driver_name = "ecb-aes-sl3516",
 			.cra_priority = 400,
 			.cra_blocksize = AES_BLOCK_SIZE,
-			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER |
+			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER | CRYPTO_ALG_KERN_DRIVER_ONLY |
+				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
+			.cra_ctxsize = sizeof(struct sl3516_ce_cipher_tfm_ctx),
+			.cra_module = THIS_MODULE,
+			.cra_alignmask = 0xf,
+			.cra_init = sl3516_ce_cipher_init,
+			.cra_exit = sl3516_ce_cipher_exit,
+		},
+		.min_keysize	= AES_MIN_KEY_SIZE,
+		.max_keysize	= AES_MAX_KEY_SIZE,
+		.setkey		= sl3516_ce_aes_setkey,
+		.encrypt	= sl3516_ce_skencrypt,
+		.decrypt	= sl3516_ce_skdecrypt,
+	}
+},
+{
+	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+	.mode = CBC_AES,
+	.alg.skcipher = {
+		.base = {
+			.cra_name = "cbc(aes)",
+			.cra_driver_name = "cbc-aes-sl3516",
+			.cra_priority = 400,
+			.cra_blocksize = AES_BLOCK_SIZE,
+			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER | CRYPTO_ALG_KERN_DRIVER_ONLY |
 				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
 			.cra_ctxsize = sizeof(struct sl3516_ce_cipher_tfm_ctx),
 			.cra_module = THIS_MODULE,
@@ -233,11 +346,110 @@
 		},
 		.min_keysize	= AES_MIN_KEY_SIZE,
 		.max_keysize	= AES_MAX_KEY_SIZE,
+		.ivsize		= AES_BLOCK_SIZE,
 		.setkey		= sl3516_ce_aes_setkey,
 		.encrypt	= sl3516_ce_skencrypt,
 		.decrypt	= sl3516_ce_skdecrypt,
 	}
 },
+{
+	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+	.mode = ECB_DES,
+	.alg.skcipher = {
+		.base = {
+			.cra_name = "ecb(des)",
+			.cra_driver_name = "ecb-des-sl3516",
+			.cra_priority = 400,
+			.cra_blocksize = DES_BLOCK_SIZE,
+			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER | CRYPTO_ALG_KERN_DRIVER_ONLY |
+				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
+			.cra_ctxsize = sizeof(struct sl3516_ce_cipher_tfm_ctx),
+			.cra_module = THIS_MODULE,
+			.cra_alignmask = 0xf,
+			.cra_init = sl3516_ce_cipher_init,
+			.cra_exit = sl3516_ce_cipher_exit,
+		},
+		.min_keysize	= DES_KEY_SIZE,
+		.max_keysize	= DES_KEY_SIZE,
+		.setkey		= sl3516_ce_des_setkey,
+		.encrypt	= sl3516_ce_skencrypt,
+		.decrypt	= sl3516_ce_skdecrypt,
+	}
+},
+{
+	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+	.mode = CBC_DES,
+	.alg.skcipher = {
+		.base = {
+			.cra_name = "cbc(des)",
+			.cra_driver_name = "cbc-des-sl3516",
+			.cra_priority = 400,
+			.cra_blocksize = DES_BLOCK_SIZE,
+			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER | CRYPTO_ALG_KERN_DRIVER_ONLY |
+				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
+			.cra_ctxsize = sizeof(struct sl3516_ce_cipher_tfm_ctx),
+			.cra_module = THIS_MODULE,
+			.cra_alignmask = 0xf,
+			.cra_init = sl3516_ce_cipher_init,
+			.cra_exit = sl3516_ce_cipher_exit,
+		},
+		.min_keysize	= DES_KEY_SIZE,
+		.max_keysize	= DES_KEY_SIZE,
+		.ivsize		= DES_BLOCK_SIZE,
+		.setkey		= sl3516_ce_des_setkey,
+		.encrypt	= sl3516_ce_skencrypt,
+		.decrypt	= sl3516_ce_skdecrypt,
+	}
+},
+{
+	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+	.mode = ECB_3DES,
+	.alg.skcipher = {
+		.base = {
+			.cra_name = "ecb(des3_ede)",
+			.cra_driver_name = "ecb-des3-ede-sl3516",
+			.cra_priority = 400,
+			.cra_blocksize = DES_BLOCK_SIZE,
+			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER | CRYPTO_ALG_KERN_DRIVER_ONLY |
+				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
+			.cra_ctxsize = sizeof(struct sl3516_ce_cipher_tfm_ctx),
+			.cra_module = THIS_MODULE,
+			.cra_alignmask = 0xf,
+			.cra_init = sl3516_ce_cipher_init,
+			.cra_exit = sl3516_ce_cipher_exit,
+		},
+		.min_keysize	= DES3_EDE_KEY_SIZE,
+		.max_keysize	= DES3_EDE_KEY_SIZE,
+		.setkey		= sl3516_ce_des_setkey,
+		.encrypt	= sl3516_ce_skencrypt,
+		.decrypt	= sl3516_ce_skdecrypt,
+	}
+},
+{
+	.type = CRYPTO_ALG_TYPE_SKCIPHER,
+	.mode = CBC_3DES,
+	.alg.skcipher = {
+		.base = {
+			.cra_name = "cbc(des3_ede)",
+			.cra_driver_name = "cbc-des3-ede-sl3516",
+			.cra_priority = 400,
+			.cra_blocksize = DES_BLOCK_SIZE,
+			.cra_flags = CRYPTO_ALG_TYPE_SKCIPHER | CRYPTO_ALG_KERN_DRIVER_ONLY |
+				CRYPTO_ALG_ASYNC | CRYPTO_ALG_NEED_FALLBACK,
+			.cra_ctxsize = sizeof(struct sl3516_ce_cipher_tfm_ctx),
+			.cra_module = THIS_MODULE,
+			.cra_alignmask = 0xf,
+			.cra_init = sl3516_ce_cipher_init,
+			.cra_exit = sl3516_ce_cipher_exit,
+		},
+		.min_keysize	= DES3_EDE_KEY_SIZE,
+		.max_keysize	= DES3_EDE_KEY_SIZE,
+		.ivsize		= DES_BLOCK_SIZE,
+		.setkey		= sl3516_ce_des_setkey,
+		.encrypt	= sl3516_ce_skencrypt,
+		.decrypt	= sl3516_ce_skdecrypt,
+	}
+},
 };
 
 #ifdef CONFIG_CRYPTO_DEV_SL3516_DEBUG
@@ -257,6 +469,7 @@
 	seq_printf(seq, "fallback modulo16 %lu\n", ce->fallback_mod16);
 	seq_printf(seq, "fallback align16 %lu\n", ce->fallback_align16);
 	seq_printf(seq, "fallback not same len %lu\n", ce->fallback_not_same_len);
+	seq_printf(seq, "fallback other %lu\n", ce->fallback_other);
 
 	for (i = 0; i < ARRAY_SIZE(ce_algs); i++) {
 		if (!ce_algs[i].ce)
@@ -268,6 +481,12 @@
 				   ce_algs[i].alg.skcipher.base.cra_name,
 				   ce_algs[i].stat_req, ce_algs[i].stat_fb);
 			break;
+		case CRYPTO_ALG_TYPE_AHASH:
+			seq_printf(seq, "%s %s reqs=%lu\n",
+				   ce_algs[i].alg.hash.halg.base.cra_driver_name,
+				   ce_algs[i].alg.hash.halg.base.cra_name,
+				   ce_algs[i].stat_req);
+			break; 
 		}
 	}
 	return 0;
@@ -295,6 +514,17 @@
 				return err;
 			}
 			break;
+		case CRYPTO_ALG_TYPE_AHASH:
+			dev_info(ce->dev, "DEBUG: Register %s\n",
+				 ce_algs[i].alg.hash.halg.base.cra_name);
+			err = crypto_register_ahash(&ce_algs[i].alg.hash);
+			if (err) {
+				dev_err(ce->dev, "Fail to register %s\n",
+					ce_algs[i].alg.hash.halg.base.cra_name);
+				ce_algs[i].ce = NULL;
+				return err;
+			}
+			break; 
 		default:
 			ce_algs[i].ce = NULL;
 			dev_err(ce->dev, "ERROR: tried to register an unknown algo\n");
@@ -316,6 +546,11 @@
 				 ce_algs[i].alg.skcipher.base.cra_name);
 			crypto_unregister_skcipher(&ce_algs[i].alg.skcipher);
 			break;
+		case CRYPTO_ALG_TYPE_AHASH:
+			dev_info(ce->dev, "Unregister %d %s\n", i,
+				 ce_algs[i].alg.hash.halg.base.cra_name);
+			crypto_unregister_ahash(&ce_algs[i].alg.hash);
+			break; 
 		}
 	}
 }
